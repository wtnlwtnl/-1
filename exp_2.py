import numpy as np
import time
from collections import Counter

def read_dataset(file_path):
    """è¯»å–å¹¶å¤„ç†æ•°æ®é›†"""
    data = np.loadtxt(file_path)
    features = data[:, :256]  # å›¾åƒç‰¹å¾
    labels_onehot = data[:, 256:]  # ç‹¬çƒ­ç¼–ç æ ‡ç­¾
    labels = np.argmax(labels_onehot, axis=1)  # è½¬ä¸ºæ•°å­—æ ‡ç­¾
    return features, labels

def compute_distances(test_data, train_data):
    """è®¡ç®—æµ‹è¯•æ•°æ®ä¸è®­ç»ƒæ•°æ®é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»"""
    diff = test_data[:, np.newaxis] - train_data
    distances = np.sqrt(np.sum(diff ** 2, axis=2))
    return distances

def classify_knn(distance_matrix, train_labels, k_neighbors):
    """åŸºäºè·ç¦»çŸ©é˜µçš„kNNåˆ†ç±»"""
    num_test = distance_matrix.shape[0]
    results = np.zeros(num_test, dtype=int)
    
    for idx in range(num_test):
        # è·å–kä¸ªæœ€è¿‘é‚»çš„ç´¢å¼•
        neighbor_idx = np.argpartition(distance_matrix[idx], k_neighbors)[:k_neighbors]
        neighbor_labels = train_labels[neighbor_idx]
        # æŠ•ç¥¨å†³å®šç±»åˆ«
        vote_count = Counter(neighbor_labels)
        results[idx] = vote_count.most_common(1)[0][0]
    
    return results

def cross_validation_loo(data_x, data_y, k_val):
    """æ‰§è¡Œç•™ä¸€æ³•äº¤å‰éªŒè¯"""
    sample_count = len(data_x)
    correct_count = 0
    
    for i in range(sample_count):
        # æ„å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
        test_x = data_x[i:i+1]
        train_x = np.vstack([data_x[:i], data_x[i+1:]])
        train_y = np.hstack([data_y[:i], data_y[i+1:]])
        
        # è®¡ç®—è·ç¦»å¹¶åˆ†ç±»
        dist = compute_distances(test_x, train_x)
        pred = classify_knn(dist, train_y, k_val)[0]
        
        if pred == data_y[i]:
            correct_count += 1
    
    return correct_count / sample_count

def evaluate_test_set(train_x, train_y, test_x, test_y, optimal_k):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    dist_matrix = compute_distances(test_x, train_x)
    predictions = classify_knn(dist_matrix, train_y, optimal_k)
    accuracy = np.mean(predictions == test_y)
    return accuracy

def main():
    # æ•°æ®åŠ è½½
    try:
        train_features, train_labels = read_dataset('semeion_train.txt')
        test_features, test_labels = read_dataset('semeion_test.txt')
    except FileNotFoundError as e:
        print(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return
    
    # å‚æ•°è®¾ç½®
    k_candidates = [1, 3, 5, 7, 9, 11, 13, 15]
    accuracy_scores = []
    
    # LOOéªŒè¯
    for k in k_candidates:
        score = cross_validation_loo(train_features, train_labels, k)
        accuracy_scores.append(score)
    
    # æ‰¾åˆ°æœ€ä½³kå€¼
    optimal_idx = np.argmax(accuracy_scores)
    best_k = k_candidates[optimal_idx]
    best_score = accuracy_scores[optimal_idx]
    
    # è¾“å‡ºç»“æœ
    print('=' * 60)
    print('è®­ç»ƒé›†LOOéªŒè¯ç»“æœæ±‡æ€»')
    print('=' * 60)
    print('kå€¼\tå‡†ç¡®ç‡\t\tç™¾åˆ†æ¯”')
    print('-' * 40)
    
    for i, (k, acc) in enumerate(zip(k_candidates, accuracy_scores)):
        star = ' â­' if i == optimal_idx else ''
        print(f'{k}\t{acc:.4f}\t\t{acc*100:.2f}%{star}')
    
    print('\n' + '=' * 60)
    print('è®­ç»ƒé›†æœ€ä¼˜ç»“æœ')
    print('=' * 60)
    print(f'ğŸ† æœ€ä½³kå€¼: {best_k}')
    print(f'ğŸ¯ æœ€é«˜å‡†ç¡®ç‡: {best_score:.4f} ({best_score*100:.2f}%)')
    
    # æµ‹è¯•é›†éªŒè¯ï¼ˆå¯é€‰ï¼‰
    test_accuracy = evaluate_test_set(train_features, train_labels, test_features, test_labels, best_k)
    print(f'ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)')

if __name__ == '__main__':
    main()